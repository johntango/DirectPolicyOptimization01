{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johntango/DirectPolicyOptimization01/blob/main/DPOTransformerExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxF96TqSmBdA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFkurn2XmHRd"
      },
      "source": [
        "# Direct Preference Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ANnaaUamLKW",
        "outputId": "24aa9464-0b2c-4697-cdc4-2e53cbec7cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# üìò SECTION 1: Install Dependencies\n",
        "!pip install -q transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4kfYIWxxmm24"
      },
      "outputs": [],
      "source": [
        "# üìò SECTION 2: Imports and Setup\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "user_info = api.whoami()\n",
        "print(user_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "secret_value = userdata.get('HF_TOKEN')\n",
        "print(secret_value)  \n",
        "\n",
        "#save the secret as an environment variable\n",
        "\n",
        "secret_value = userdata.get('HF_TOKEN')\n",
        "print(secret_value)\n",
        "os.environ['HF_TOKEN'] = secret_value\n",
        "secret_value = userdata.get('HF_TOKENWRITE')\n",
        "os.environ['HF_TOKENWRITE'] = secret_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Mk0rpKLZmrkv"
      },
      "outputs": [],
      "source": [
        "# üìò SECTION 3: Load Model and Tokenizer\n",
        "model_name = \"gpt2\"  # Replace with instruction-tuned model if desired\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2ESGanZ6mv4j"
      },
      "outputs": [],
      "source": [
        "# üìò SECTION 4: Tokenization Helper\n",
        "def tokenize_pair(prompt, response, max_length=512):\n",
        "    tokenized = tokenizer(prompt + response, truncation=True, max_length=max_length,\n",
        "                          padding=\"max_length\", return_tensors=\"pt\")\n",
        "    return {k: v.to(device) for k, v in tokenized.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "G3F3dXahmz9C"
      },
      "outputs": [],
      "source": [
        "# üìò SECTION 5: DPO Loss Function\n",
        "def dpo_loss(chosen_logps, rejected_logps, beta=0.1):\n",
        "    diff = (chosen_logps - rejected_logps) / beta\n",
        "    return -F.logsigmoid(diff).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3Jxx8ox3m4ST"
      },
      "outputs": [],
      "source": [
        "# üìò SECTION 6: Compute Log Probability of Sequence\n",
        "def compute_logprob(model, input_ids, attention_mask):\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "    return -outputs.loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpKImo2mm8Ps",
        "outputId": "505cb51d-2019-4530-aad4-de71bea36061"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': 'I was wondering if you could walk me through the process of setting up a hydroponic garden for herbs.',\n",
              " 'chosen': \"Sure! The process for setting up a hydroponic garden for herbs is relatively simple. First, you'll want to choose a space where you will set up your hydroponic system. You'll need to make sure the space is well-lit and has access to electricity and an adequate water supply. Next, you'll need to choose the type of hydroponic system you want to use. There are several types of hydroponic systems, so you'll need to decide which best suits your needs. Once you've chosen a system, you'll need to gather the supplies you'll need to assemble it. This includes things like pumps, growing trays, grow lights, and nutrients. Once you've assembled the system, you'll need to add your choice of herbs to the system. Lastly, you'll need to monitor and adjust the system as needed to ensure your herbs are getting the correct amount of light, water, and nutrients.\",\n",
              " 'rejected': 'How do I store a bagels for eating at a later date?\\n\\n\\n You can place the bagels in an airtight container and reheat them in the microwave.  Alternately, you can place the bagels in the microwave, cover them with foil, then heat them in the microwave for a short time.'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"Dahoas/synthetic-instruct-gptj-pairwise\", split=\"train[:500]\")\n",
        "dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0PgQata_m_iZ"
      },
      "outputs": [],
      "source": [
        "# üìò SECTION 8: Dataloader for Mini-batching\n",
        "def collate_fn(samples):\n",
        "    return samples\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFkV_LDQnDMZ",
        "outputId": "04c7b1ef-b2aa-4cf5-98d4-218a87ffa25e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:40<00:00,  3.06it/s, loss=0.00271]\n",
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:41<00:00,  3.04it/s, loss=0.000474]\n",
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:40<00:00,  3.06it/s, loss=1.81e-5]\n"
          ]
        }
      ],
      "source": [
        "# üìò SECTION 9: Training Loop\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(3):\n",
        "    loop = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
        "    for batch in loop:\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for sample in batch:\n",
        "            prompt = sample[\"prompt\"]\n",
        "            chosen = sample[\"chosen\"]\n",
        "            rejected = sample[\"rejected\"]\n",
        "\n",
        "            chosen_input = tokenize_pair(prompt, chosen)\n",
        "            rejected_input = tokenize_pair(prompt, rejected)\n",
        "\n",
        "            chosen_logp = compute_logprob(model, **chosen_input)\n",
        "            rejected_logp = compute_logprob(model, **rejected_input)\n",
        "\n",
        "            loss = dpo_loss(chosen_logp, rejected_logp)\n",
        "            total_loss += loss\n",
        "\n",
        "        avg_loss = total_loss / len(batch)\n",
        "        optimizer.zero_grad()\n",
        "        avg_loss.backward()\n",
        "        optimizer.step()\n",
        "        loop.set_postfix(loss=avg_loss.item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SCuWTrcnFgM",
        "outputId": "0a55e4de-5572-48a6-db51-78eba8164266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model saved to 'dpo-finetuned-model'\n"
          ]
        }
      ],
      "source": [
        "# üìò SECTION 10: Save Fine-Tuned Model\n",
        "model.save_pretrained(\"./sample_data/dpo-finetuned-model\")\n",
        "tokenizer.save_pretrained(\"./sample_data/dpo-finetuned-model\")\n",
        "print(\"‚úÖ Model saved to './sample_data/dpo-finetuned-model'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now Run the Trained Model\n",
        "#\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Path to your fine-tuned model directory\n",
        "model_path = \"./sample_data/dpo-finetuned-model\"\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load your fine-tuned model\n",
        "model_path = \"./sample_data/dpo-finetuned-model\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Set a real pad token (GPT2 has none by default)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "model.resize_token_embeddings(len(tokenizer))  # Resize if tokenizer changed\n",
        "model.to(device)\n",
        "\n",
        "# Sample input\n",
        "prompt = \"The future of quantum computing is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Create attention mask\n",
        "attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"].to(device),\n",
        "        attention_mask=attention_mask.to(device),\n",
        "        max_length=100,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "# Decode and print\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPnZ6o2Kt0Jz0fGkruWoe7w",
      "gpuType": "A100",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
